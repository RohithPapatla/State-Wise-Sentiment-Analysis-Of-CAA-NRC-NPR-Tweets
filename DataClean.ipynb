{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 5)\n",
      "Index(['Location', 'Retweet_Count', 'Date', 'Tweet', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Data = pd.read_excel(\"D:/Information Retrieval/Project/IR_data.xlsx\")\n",
    "print(Data.shape)\n",
    "print(Data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 5)\n"
     ]
    }
   ],
   "source": [
    "Data['Tweet'].value_counts().to_numpy()\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = [\"\\U0001F600\",\"\\U0001F603\",\"\\U0001F604\",\"\\U0001F601\",\"\\U0001F606\",\"\\U0001F605\",\"\\U0001F923\",\"\\U0001F602\",\"\\U0001F642\",\n",
    "          \"\\U0001F643\",\"\\U0001F609\",\"\\U0001F60A\",\"\\U0001F607\",\"\\U0001F970\",\"\\U0001F60D\",\"\\U0001F929\", \"\\U0001F618\" \"\\U0001F617\",\n",
    "          \"\\U0001F61A\",\"\\U0001F619\",\"\\U0001F60B\",\"\\U0001F61B\",\"\\U0001F61C\",\"\\U0001F92A\",\"\\U0001F61D\",\"\\U0001F911\",\"\\U0001F917\",\n",
    "         \"\\U0001F92D\",\"\\U0001F92B\",\"\\U0001F914\",\"\\U0001F910\",\"\\U0001F928\",\"\\U0001F610\",\"\\U0001F611\",\"\\U0001F636\",\"\\U00001F60F\",\n",
    "          \"\\U0001F612\",\"\\U0001F644\",\"\\U0001F62C\",\"\\U0001F925\",\"\\U0001F60C\",\"\\U0001F614\",\"\\U0001F62A\",\"\\U0001F924\",\"\\U0001F634\",\n",
    "          \"\\U0001F637\",\"\\U0001F912\",\"\\U0001F915\",\"\\U0001F922\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(Data):\n",
    "    for k in range(len(Data)):\n",
    "    \n",
    "        x = Data.iloc[k,3]\n",
    "        words = x.split()\n",
    "        for word in words:\n",
    "            if(word[0] == \"@\"):\n",
    "                Data.iloc[k,3] = Data.iloc[k, 3].replace(word, \"\")\n",
    "    \n",
    "    return Data\n",
    "        \n",
    "\n",
    "Data = remove_references(Data)\n",
    "Data = Data.to_numpy()\n",
    "#print(type(Data))\n",
    "#print(Data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = {\n",
    "    \"Ã¢â‚¬Â¦\":\"\", \"â‚¬\":\"€\", \"â€š\":\"‚\", \"â€ž\":\"„\", \"â€¦\":\"…\", \"Ë†\":\"ˆ\",\"â€¹\":\"‹\", \"â€˜\":\"‘\", \"â€™\":\"’\", \"â€œ\":\"“\", \"â€\":\"”\",\n",
    "    \"â€¢\":\"•\", \"â€“\":\"–\", \"â€”\":\"—\", \"Ëœ\":\"˜\", \"â„¢\":\"™\",\"â€º\":\"›\", \"Å“\":\"œ\", \"Å’\":\"Œ\", \"Å¾\":\"ž\", \"Å¸\":\"Ÿ\",\"Å¡\":\"š\",\n",
    "    \"Å½\":\"Ž\", \"Â¡\":\"¡\", \"Â¢\":\"¢\", \"Â£\":\"£\",\"Â¤\":\"¤\", \"Â¥\":\"¥\", \"Â¦\":\"¦\", \"Â§\":\"§\", \"Â¨\":\"¨\",\"Â©\":\"©\", \"Âª\":\"ª\", \n",
    "    \"Â«\":\"«\", \"Â¬\":\"¬\", \"Â®\":\"®\",\"Â¯\":\"¯\", \"Â°\":\"°\", \"Â±\":\"±\", \"Â²\":\"²\", \"Â³\":\"³\",\"Â´\":\"´\", \"Âµ\":\"µ\", \"Â¶\":\"¶\",\n",
    "    \"Â·\":\"·\", \"Â¸\":\"¸\",\"Â¹\":\"¹\", \"Âº\":\"º\", \"Â»\":\"»\", \"Â¼\":\"¼\", \"Â½\":\"½\",\"Â¾\":\"¾\", \"Â¿\":\"¿\", \"Ã€\":\"À\", \"Ã‚\":\"Â\",\n",
    "    \"Ãƒ\":\"Ã\",\"Ã„\":\"Ä\", \"Ã…\":\"Å\", \"Ã†\":\"Æ\", \"Ã‡\":\"Ç\", \"Ãˆ\":\"È\",\"Ã‰\":\"É\", \"ÃŠ\":\"Ê\", \"Ã‹\":\"Ë\", \"ÃŒ\":\"Ì\", \"ÃŽ\":\"Î\",\n",
    "    \"Ã‘\":\"Ñ\", \"Ã’\":\"Ò\", \"Ã“\":\"Ó\", \"Ã”\":\"Ô\", \"Ã•\":\"Õ\",\"Ã–\":\"Ö\", \"Ã—\":\"×\", \"Ã˜\":\"Ø\", \"Ã™\":\"Ù\", \"Ãš\":\"Ú\",\"Ã›\":\"Û\", \n",
    "    \"Ãœ\":\"Ü\", \"Ãž\":\"Þ\", \"ÃŸ\":\"ß\", \"Ã¡\":\"á\",\"Ã¢\":\"â\", \"Ã£\":\"ã\", \"Ã¤\":\"ä\", \"Ã¥\":\"å\", \"Ã¦\":\"æ\",\"Ã§\":\"ç\", \"Ã¨\":\"è\",\n",
    "    \"Ã©\":\"é\", \"Ãª\":\"ê\", \"Ã«\":\"ë\",\"Ã¬\":\"ì\", \"Ã­\":\"í\", \"Ã®\":\"î\", \"Ã¯\":\"ï\", \"Ã°\":\"ð\",\"Ã±\":\"ñ\", \"Ã²\":\"ò\", \"Ã³\":\"ó\",\n",
    "    \"Ã´\":\"ô\", \"Ãµ\":\"õ\",\"Ã¶\":\"ö\", \"Ã·\":\"÷\", \"Ã¸\":\"ø\", \"Ã¹\":\"ù\", \"Ãº\":\"ú\",\"Ã»\":\"û\", \"Ã¼\":\"ü\", \"Ã½\":\"ý\", \"Ã¾\":\"þ\", \"Ã¿\":\"ÿ\",\n",
    "    \"Ã¢â‚¬Â¦\":\" \", \"Ã°Ã¿â€¡Â¨Ã°Ã¿â€¡Â¦\":\"\", \"Ã¢â‚¬â„¢\":\"’\" , \"aÃ¢â‚¬Â¦\":\"\",\"Ã¢â‚¬Â¦\":\" \", \" Ã¢â‚¬Å“\":\" \", \"Ã¢â‚¬â„¢s\":\"'s\",\n",
    "    \"â‚¬\":\"\", \"â„¢\":\"'\",\"Ã¢\":\"\", \"Ã¢â‚¬Â¦\":\"\", \"Ã°\":\"\", \"Ã¿\":\"\", \"‘Â\\x8f‘Â\\x8f‘Â\\x8d\":\"\", \"˜¡\":\"\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(data, special_characters):\n",
    "    for k in range(data.shape[0]):\n",
    "        for l in range(data.shape[1]):\n",
    "            x = data[k][l]\n",
    "            #print(x)\n",
    "    \n",
    "            if(isinstance(x, str)):\n",
    "                words = x.split()\n",
    "                #print(words)\n",
    "            \n",
    "                for i in range(len(words)):\n",
    "                    word = words[i]\n",
    "                    for char in special_characters:\n",
    "                        if(char in word):\n",
    "                            #print(char)\n",
    "                            word = word.replace(char, special_characters[char])\n",
    "                        \n",
    "                    words[i] = word\n",
    "                new_words = ' '.join([str(elem) for elem in words])\n",
    "                data[k][l] = new_words\n",
    "                \n",
    "    return data\n",
    "\n",
    "Data = remove_special_characters(Data, special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary():\n",
    "    data = open(\"D:/Information Retrieval/Project/abbreviations.txt\", \"r\")\n",
    "    data = data.readlines()\n",
    "    \n",
    "    abbreviations = {}\n",
    "    for string in data:\n",
    "    \n",
    "        words = string.split(\"=\")\n",
    "        #print(words[0], words[1])\n",
    "        x = words[1][:-1]\n",
    "        abbreviations[words[0]] = x\n",
    "        \n",
    "    return abbreviations\n",
    "\n",
    "abbreviations = create_dictionary()\n",
    "#print(abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_abbreviations(data, abbreviations):\n",
    "    \n",
    "    for k in range(data.shape[0]):\n",
    "        for l in range(data.shape[1]):\n",
    "            user_string = data[k][l]\n",
    "            \n",
    "            if(isinstance(user_string, str)):\n",
    "                words = user_string.split()\n",
    "                \n",
    "                for i in range(len(words)):\n",
    "                    \n",
    "                    word = words[i]\n",
    "                    if(word in abbreviations):\n",
    "                        word = abbreviations[word]\n",
    "                    \n",
    "                    words[i] = word\n",
    "                new_words = ' '.join([str(elem) for elem in words])\n",
    "                data[k][l] = new_words\n",
    "    return data\n",
    "        \n",
    "Data = remove_abbreviations(Data, abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace = ' \\t\\n\\r\\v\\f'\n",
    "ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n",
    "ascii_uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "ascii_letters = ascii_lowercase + ascii_uppercase\n",
    "digits = '0123456789'\n",
    "hexdigits = digits + 'abcdef' + 'ABCDEF'\n",
    "octdigits = '01234567'\n",
    "punctuation = r\"\"\"!\"#$%&'()*+-./:;<=>?[\\]^_`{|}~\"\"\"\n",
    "printable = digits + ascii_letters + punctuation + whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_locations(Data):\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        \n",
    "        new_location = \"\"\n",
    "        location = data[i][0]\n",
    "        if(isinstance(location, str)):\n",
    "            #print(location)\n",
    "            for k in location:\n",
    "                if(k in printable):\n",
    "                    new_location = new_location + k\n",
    "        \n",
    "        data[i][0] = new_location\n",
    "    \n",
    "    return data\n",
    "        \n",
    "#modify_locations(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Case Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower(data):\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            \n",
    "            if(isinstance(data[i][j], str)):\n",
    "                data[i][j] = data[i][j].lower()\n",
    "        \n",
    "        if(data[i][0] == ''):\n",
    "            data[i][0] = 'random'\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "Data = convert_lower(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Tag Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(data):\n",
    "    \n",
    "    punctuations = '''!()-![]{};:+'\"\\,<>./?#$%^&*_~'''\n",
    "    for i in range(data.shape[0]):\n",
    "        tweet = data[i][3]\n",
    "        \n",
    "        new_tweet = \"\"\n",
    "        for k in tweet:\n",
    "            if(punctuation.find(k) == -1):\n",
    "                new_tweet = new_tweet + k\n",
    "        data[i][3] = new_tweet\n",
    "    \n",
    "    return data\n",
    "        \n",
    "Data = remove_hashtags(Data)\n",
    "#print(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        tweet = data[i][3]\n",
    "        \n",
    "        x = tweet.split()\n",
    "        \n",
    "        new_tweet = \"\"\n",
    "        for k in x:\n",
    "            new_tweet = new_tweet + \" \" + (lemmatizer.lemmatize(k))\n",
    "        data[i][3] = new_tweet\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = lemmatization(Data)\n",
    "#print(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_states(states, data):\n",
    "    x = []\n",
    "    for state in states:\n",
    "        if(state not in data):\n",
    "            cities = states[state]\n",
    "            for city in cities:\n",
    "                if(city in data):\n",
    "                    x.append(state)\n",
    "            for city in cities:\n",
    "                for k in data:\n",
    "                    new_data = city + \"*\"\n",
    "                    if(re.search(new_data, k)):\n",
    "                        x.append(state)\n",
    "        else:\n",
    "            x.append(state)\n",
    "\n",
    "    for state in states:\n",
    "        for k in data:\n",
    "            if(re.search((state + \"+\"), k)):\n",
    "                x.append(state)\n",
    "                \n",
    "    new_x = []\n",
    "    for k in x:\n",
    "        if(k not in new_x):\n",
    "            new_x.append(k)\n",
    "    \n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['himachal pradesh' 0 'sat feb 29 07:54:35 +0000 2020'\n",
      "  ' people professing nationalism, patriotism and those who are pro or anti caa, nrc, npr and think people out of there physical boundary are their enemy are more like dog in mentality because ilaaqe to kutton ke hote hain'\n",
      "  1]\n",
      " ['himachal pradesh' 0 'sat feb 29 03:20:05 +0000 2020'\n",
      "  ' no hate at all no nrc now in country then why you are on street and vandalising across country actually you want another papistan you r better here with u but altafhossain, mqm leader ,descendents of propaki'\n",
      "  1]\n",
      " ['himachal pradesh' 0 'sat feb 29 03:13:22 +0000 2020'\n",
      "  ' i understand your emotion, but feel that firing them doe not solve the core problem fully spreading the truth of caa npr and nrc to the people of india is a better and effective approach to the issue related to caanrcnpr i sti'\n",
      "  0]\n",
      " ...\n",
      " ['uttarakhand' 2 'thu feb 20 17:09:45 +0000 2020'\n",
      "  ' rt right or left, wing are meant to keep u afloat happy flying though, the predominant and positive thought shall al…'\n",
      "  0]\n",
      " ['uttarakhand' 2 'thu feb 20 10:58:43 +0000 2020'\n",
      "  ' right or left, wing are meant to keep u afloat happy flying though, the predominant and positive thought shall always be right, left is an alternative thought caanrcprotests caanrcnpr caa leftist rightmatters'\n",
      "  0]\n",
      " ['uttarakhand' 0 'mon mar 02 10:47:37 +0000 2020'\n",
      "  ' we have lost some wonderful writers, actors, musicians, comedians, painter and politician to activism mondaymotivation mondaythoughts mondaymorning caa caanrcprotests lifelovekumbh'\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "print(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = Data, columns=[\"Location\", \"Retweet Count\", \"Date\", \"Tweet\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('D:/Information Retrieval/Project/Final_Data/Test_Data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-wise Division Of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def state_wise_tweets(data):\n",
    "    states = {'andhra pradesh': ['hyderabad','adilabad','anantapur','chittoor','kakinada','guntur','karimnagar','khammam',\n",
    "                                 'krishna','kurnool','mahbubnagar','medak','nalgonda','nizamabad','ongole','srikakulam',\n",
    "                                'nellore','visakhapatnam','vizianagaram','warangal','eluru','kadapa'], \n",
    "              \n",
    "              'arunachal pradesh': ['itangar','anjaw','changlang','east siang','kurung kumey','lohit','lower dibang valley',\n",
    "                                    'lower subansiri','papum pare','tawang','tirap','dibang valley','upper siang',\n",
    "                                    'upper subansiri','west kameng' 'west siang'],\n",
    "              \n",
    "              'assam': ['dispur','guwahati','baksa','barpeta','bongaigaon','cachar','chirang','darrang','dhemaji',\n",
    "                        'dima hasao','dhubri','dibrugarh','goalpara','golaghat','hailakandi','jorhat','kamrup',\n",
    "                        'kamrup metropolitan','karbi anglong','karimganj','kokrajhar','lakhimpur','marigaon','nagaon',\n",
    "                        'nalbari','sibsagar','sonitpur','tinsukia','udalguri'], \n",
    "              \n",
    "              'bihar': ['patna','gaya','bhagalpur','muzaffarpur','purnia','darbhanga','arrah','begusarai','katihar','munger',\n",
    "                       'chhapra','danapur','bettiah','saharsa','sasaram','hajipur','dehri','siwan','motihari','nawada',\n",
    "                       'bagaha','buxar','kishanganj','sitamarhi','jamalpur','jehanabad','aurangabad'], \n",
    "              \n",
    "              'chhattisgarh': ['raipur','bastar','bijapur','bilaspur','dantewada','dhamtari','durg','jashpur','janjgir-champa'\n",
    "                        'korba','koriya','kanker','kabirdham','mahasamund','narayanpur','raigarh','rajnandgaon','surguja'],\n",
    "              \n",
    "              'goa': ['panaji'],\n",
    "              \n",
    "              'gujarat': ['gandhinagar','ahmedabad','amreli district','anand','banaskantha','bharuch','bhavnagar','dahod',\n",
    "                        'the dangs','jamnagar','junagadh','kutch','kheda','mehsana','narmada','navsari','patan','panchmahal',\n",
    "                        'porbandar','rajkot','sabarkantha','surendranagar','surat','vyara','vadodara','valsad'],\n",
    "              \n",
    "              'haryana': ['chandigarh','ambala','bhiwani','faridabad','fatehabad','gurgaon','hissar','jhajjar','jind',\n",
    "                          'karnal','kaithal','kurukshetra','mahendragarh','mewat','palwal','panchkula','panipat','rewari',\n",
    "                          'rohtak','sirsa','sonipat','yamuna Nagar'],\n",
    "              \n",
    "              'himachal Pradesh':['shimla','bilaspur','chamba','hamirpur','kangra','kinnaur','kullu','lahaul','spiti','mandi',\n",
    "                                'sirmaur','solan','una'],\n",
    "              \n",
    "              'jammu and kashmir': ['srinagar','anantnag','badgam','bandipora','baramulla','doda','ganderbal','jammu','kargil',\n",
    "                        'kathua','kishtwar','kupwara','kulgam','leh','poonch','pulwama','rajauri','ramban','reasi','samba',\n",
    "                        'shopian','srinagar','udhampur'],\n",
    "              \n",
    "              'jharkhand': ['ranchi','bokaro','chatra','deoghar','dhanbad','dumka','east singhbhum','garhwa','giridih','godda',\n",
    "                        'gumla','hazaribag','jamtara','khunti','koderma','latehar','lohardaga','pakur','palamu','ramgarh',\n",
    "                        'ranchi','sahibganj','seraikela','simdega','west-singhbhum'],\n",
    "              \n",
    "              'karnataka': ['bangalore','bengaluru','bagalkot','belgaum','bellary','bidar','bijapur','chamarajnagar','chikkamagaluru',\n",
    "                        'chikkaballapur','chitradurga','davanagere','dharwad','dakshinakannada','gadag','gulbarga','hassan','haveri_district',\n",
    "                            'kodagu','kolar','koppal','mandya','mysore','raichur','shimoga','tumkur','udupi','ramanagara','yadgir'],\n",
    "              \n",
    "              'kerala': ['thiruvananthapuram','alappuzha','ernakulam','idukki','kannur','kasaragod','kollam','kottayam',\n",
    "                    'kozhikode','malappuram','palakkad','pathanamthitta','thrissur','thiruvananthapuram','Wayanad'],\n",
    "              \n",
    "              'madhya pradesh': ['bhopal','alirajpur','anuppur','ashoknagar','balaghat','barwani','betul','bhind','burhanpur',\n",
    "                    'chhatarpur','chhindwara','damoh','datia','dewas','dhar','dindori','guna','gwalior','harda','hoshangabad',\n",
    "                    'indore','jabalpur','jhabua','katni','khandwa','khargone','mandla','mandsaur','morena','narsinghpur','neemuch',\n",
    "                    'panna','rewa','rajgarh','ratlam','raisen','sagar','satna','sehore','seoni','shahdol','shajapur',\n",
    "                    'sheopur','shivpuri','sidhi','singrauli','tikamgarh','ujjain','umaria','vidisha'],\n",
    "              \n",
    "              'maharashtra': ['mumbai','pune','ahmednagar','akola','amravati','aurangabad','bhandara','beed','buldhana','chandrapur','dhule',\n",
    "                              'gadchiroli','gondia','hingoli','jalgaon','jalna','kolhapur','latur','nandurbar','nanded','nagpur','nashik',\n",
    "                              'osmanabad','parbhani','raigad','ratnagiri','sindhudurg','sangli','solapur','satara','thane','wardha','Washim','yavatmal'],\n",
    "              \n",
    "              'manipur': ['imphal','bishnupur','churachandpur','chandel','senapati','tamenglong','thoubal','ukhrul'],\n",
    "              \n",
    "              'meghalaya': ['shillong','east garo hills','east Khasi Hills','Jaintia Hills','Ri Bhoi','South Garo Hills',\n",
    "                    'West Garo Hills','West Khasi Hills'],\n",
    "              \n",
    "              'mizoram': ['aizawi','champhai','kolasib','lawngtlai','lunglei','mamit','saiha','serchhip'],\n",
    "              \n",
    "              'nagaland':['kohima','dimapur','mokokchung','mon','phek','tuensang','wokha','zunheboto'],\n",
    "              \n",
    "              'orissa':['bhubaneshwar','angul','boudh','bhadrak','balangir','bargarh','baragarh','balasore','cuttack','debagarh',\n",
    "                    'deogarh','dhenkanal','ganjam','gajapati','jharsuguda','jajpur','jagatsinghpur','khordha','kendujhar', 'keonjhar',\n",
    "                    'kalahandi','kandhamal','koraput','kendrapara','malkangiri','mayurbhanj','nabarangpur','nuapada','nayagarh',\n",
    "                    'puri','rayagada','sambalpur','subarnapur','sonepur','sundergarh'],\n",
    "              \n",
    "              'punjab':['chandigarh','amritsar','barnala','bathinda','firozpur','faridkot','fatehgarhsahib','fatehgarh','fazilka',\n",
    "                    'gurdaspur','hoshiarpur','jalandhar','kapurthala','ludhiana','mansa','moga','pathankot','patiala','rupnagar',\n",
    "                    'ajitgarh','mohali','sangrur','nawanshahr'],\n",
    "              \n",
    "              'rajasthan':['jaipur','ajmer','alwar','bikaner','barmer','banswara','bharatpur','baran','bundi','bhilwara','churu',\n",
    "                    'chittorgarh','dausa','dholpur','dungapur','ganganagar','hanumangarh','jhunjhunu','jalore','jodhpur','jaipur',\n",
    "                    'jaisalmer','jhalawar','karauli','kota','nagaur','pali','pratapgarh','rajsamand','sikar','sawaimadhopur',\n",
    "                    'sirohi','tonk','udaipur'],\n",
    "              \n",
    "              'sikkim': ['gangtok'],\n",
    "              \n",
    "              'tamil Nadu': ['chennai','ariyalur','coimbatore','cuddalore','dharmapuri','dindigul','erode','kanchipuram',\n",
    "                        'kanyakumari','karur','madurai','nagapattinam','nilgiris','namakkal','perambalur','pudukkottai',\n",
    "                        'ramanathapuram','salem','sivaganga','tirupur','tiruchirappalli','theni','tirunelveli','thanjavur',\n",
    "                        'thoothukudi','tiruvallur','tiruvarur','tiruvannamalai','vellore','viluppuram','virudhunagar'],\n",
    "              \n",
    "              'telagana': ['hyderabad'],\n",
    "              \n",
    "              'tripura':['agartala','dhalai','khowai'],\n",
    "              \n",
    "              'uttarakhand': ['dehradun','almora','bageshwar','chamoli','champawat','haridwar','nainital','pauri','garhwal',\n",
    "                    'pithoragarh','rudraprayag','tehri','Udhamsinghnagar','uttarkashi'],\n",
    "              \n",
    "              'delhi':['CP','jammia','karol bagh','Mayur vihar','Preet vihar','shahdara','saket','shaheen bagh','jnu',\n",
    "                       'seelampur', 'jafrabad', 'maujpur', 'kardampuri', 'babarpur', 'gokulpuri','shivpuri'],\n",
    "              \n",
    "              'uttar Pradesh': ['lucknow','muzaffarnagar','Aligarh', 'Mau', 'Azamgarh', 'Kanpur', 'Bareilly', 'Shahjahanpur',\n",
    "                    'Ghaziabad', 'Bulandshahr','allahabad','agra','ambedkarnagar','auraiya','barabanki','budaun','bagpat',\n",
    "                    'bahraich','bijnor','ballia','banda','balrampur','basti','chandauli','chitrakoot','deoria','etah','Etawah',\n",
    "                    'firozabad','farrukhabad','fatehpur','faizabad','gonda','ghazipur','gorakhpur','hamirpur','hardoi','jhansi',\n",
    "                    'jalaun','jaunpur','kannauj','kanpur','kaushambi','kushinagar','lalitpur','meerut','maharajganj','mahoba',\n",
    "                    'mirzapur','moradabad','mainpuri','mathura','panchsheel','hapur','pilibhit','shamli','pratapgarh','rampur',\n",
    "                    'raebareli','saharanpur','sitapur','shahjahanpur','siddharthnagar','sonbhadra','sultanpur','shravasti','unnao','varanasi'],\n",
    "              \n",
    "              'west Bengal':['kolkata','birbhum','bankura','bardhaman','darjeeling','dinajpur','hooghly','howrah','jalpaiguri',\n",
    "                    'coochbehar','maldah','medinipur','murshidabad','nadia','purulia']}\n",
    "    \n",
    "    state_tweets = {}\n",
    "    a = 0\n",
    "    for i in range(data.shape[0]):\n",
    "        count = True\n",
    "        location = data[i][0].split()\n",
    "        #print(a, location)\n",
    "        \n",
    "        for state in states:\n",
    "            if(state not in location):\n",
    "                cities = states[state]\n",
    "                for city in cities:\n",
    "                    if(city in location):\n",
    "                        count = False\n",
    "                        if(state in state_tweets):\n",
    "                            #print(\"X\")\n",
    "                            old_data = state_tweets[state]\n",
    "                            old_data.append(list(data[i,1:]))\n",
    "                            state_tweets[state] = old_data\n",
    "                        else:\n",
    "                            #print(\"Y\")\n",
    "                            new_data = list(data[i,1:])\n",
    "                            x = []\n",
    "                            x.append(new_data)\n",
    "                            state_tweets[state] = x\n",
    "                        \n",
    "            else:\n",
    "                count = False\n",
    "                if(state in state_tweets):\n",
    "                    #print(\"A\")\n",
    "                    old_data = state_tweets[state]\n",
    "                    old_data.append(list(data[i,1:]))\n",
    "                    state_tweets[state] = old_data\n",
    "                else:\n",
    "                    #print(\"B\")\n",
    "                    new_data = list(data[i,1:])\n",
    "                    x = []\n",
    "                    x.append(new_data)\n",
    "                    state_tweets[state] = x\n",
    "        \n",
    "        if(count == True):\n",
    "            tweet_data = data[i][3].split()\n",
    "            new_states = return_states(states, tweet_data)\n",
    "            \n",
    "            #print(new_states)\n",
    "            if(len(new_states)!= 0):\n",
    "                count = False\n",
    "                for k in new_states:\n",
    "                    if(k in state_tweets):\n",
    "                        #print(\"C\")\n",
    "                        old_data = state_tweets[k]\n",
    "                        old_data.append(list(data[i,1:]))\n",
    "                        state_tweets[k] = old_data\n",
    "                    else:\n",
    "                        #print(\"D\")\n",
    "                        new_data = list(data[i,1:])\n",
    "                        x = []\n",
    "                        x.append(new_data)\n",
    "                        state_tweets[k] = x\n",
    "                        \n",
    "        if(count == True):\n",
    "            if('random' in state_tweets):\n",
    "                #print(\"E\")\n",
    "                old_data = state_tweets['random']\n",
    "                old_data.append(list(data[i,1:]))\n",
    "                state_tweets['random'] = old_data\n",
    "                \n",
    "            else:\n",
    "                #print(\"F\")\n",
    "                new_data = list(data[i,1:])\n",
    "                x = []\n",
    "                x.append(new_data)\n",
    "                state_tweets['random'] = x\n",
    "        a = a + 1\n",
    "    \n",
    "    return state_tweets\n",
    "        \n",
    "dict1 = state_wise_tweets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "himachal Pradesh 980\n",
      "jammu and kashmir 1879\n",
      "nagaland 792\n",
      "rajasthan 213\n",
      "tamil Nadu 231\n",
      "assam 50\n",
      "karnataka 157\n",
      "haryana 60\n",
      "maharashtra 644\n",
      "random 377\n",
      "uttarakhand 13\n",
      "delhi 882\n",
      "madhya pradesh 150\n",
      "punjab 49\n",
      "jharkhand 26\n",
      "andhra pradesh 90\n",
      "telagana 79\n",
      "uttar Pradesh 154\n",
      "bihar 107\n",
      "gujarat 64\n",
      "chhattisgarh 55\n",
      "orissa 72\n",
      "west Bengal 39\n",
      "goa 8\n",
      "kerala 24\n",
      "meghalaya 14\n",
      "arunachal pradesh 3\n",
      "tripura 1\n",
      "7213\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "for k in dict1:\n",
    "    sum1 = sum1 + len(dict1[k])\n",
    "    print(k, len(dict1[k]))\n",
    "    \n",
    "print(sum1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
